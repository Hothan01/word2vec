# 数据集中没有任何标点符号，即不用考虑去掉停用词（出现频率太高的词，如逗号，句号等等）

import numpy as np
import time

words_corpus = []    # 语料 ：存储所有的词，不管重复与否
words_diff = []   #词汇表
word_times = {}  # 词汇表以及对应的词频
word_vec = {}   # 词汇表词向量，字典类型
word_Huffman_path_vec = {}   # 叶子节点的Huffman路径节点向量
word_Huffman_path_code = {}   # 叶子节点的Huffman路径节点编码
m_length = 100   # 词向量的维度
p_learning = 0.025   # 学习率 skip模式下默认为0.025
window_length = 3   # 窗口大小
min_count = 5   # 最⼩词频阈值

class Nodes:
    def __init__(self, value):
        self.value = value  # 词频以及中间向量的词频之和
        self.word = ""  # 叶子节点存放的单词
        self.leaf = False   # 是否为叶子节点
        self.HuffmanCode = -1  # 在建树的时候，定义<0, 1>
        self.left = None   # 左孩子
        self.right = None   # 右孩子
        self.father = None   # 父亲节点
        self.weight = None # 权重向量，需要更新的syn1

    def __str__(self):
        return "词频:{} 单词:{}".format(self.value, self.word)


# 统计词频，统计无重复单词
def WordCounter():
    print("WordCounter()")

    # 读入数据集
    filename = "sentence.txt"  # 当前目录下
    fr = open(filename)
    WordLine = fr.readline()
    while WordLine:
        for item in WordLine.strip().split():
            words_corpus.append(item)
        WordLine = fr.readline()

    # 去重,以及最小词频处理
    for item in words_corpus:
        if item not in words_diff:
            time_count = words_corpus.count(item)
            if time_count > min_count:
                words_diff.append(item)  # 词汇表
                word_times[item] = time_count  # 记录词与其对应的词频

    print("语料库大小：", len(words_corpus))  # 语料库大小
    print("词汇表大小：", len(words_diff))  # 词汇表

    # 对词频进行排序（从小到大）
    wordtime = sorted(word_times.items(), key=lambda x: x[1], reverse=False)
    #   排序返回的是 包含元组的列表   [('word', 1), ('supporting', 1), ......,('conduct', 1)]

    return wordtime

#建立树节点列表，从小到大排序的
def GetNodes():
    print("GetNodes()")

    Nodes_list = []  # 存储节点的列表

    wordtime = WordCounter()

    for index in wordtime:
        value = index[1]
        TreeNode = Nodes(value)   # 建立树节点class
        TreeNode.word = index[0]
        TreeNode.leaf = True
        Nodes_list.append(TreeNode)

    return Nodes_list

# 找到节点列表中词频最小的两个节点
def min2(Nodes_list):

    Node2_list2 = [] # 收集剩下的节点列表
    mi2 = [Nodes(float('inf')), Nodes(float('inf'))]   # [最小, 较小]

    for nodetmp in Nodes_list:
        if nodetmp.value < mi2[0].value:
            if mi2[1].value != float('inf'):
                Node2_list2.append(mi2[1])
            mi2[0], mi2[1] = nodetmp, mi2[0]
        elif nodetmp.value < mi2[1].value:
            if mi2[1].value != float('inf'):
                Node2_list2.append(mi2[1])
            mi2[1] = nodetmp
        else:
            Node2_list2.append(nodetmp)

    mi2[0].HuffmanCode = 0   # 较小，右孩子
    mi2[1].HuffmanCode = 1   # 较大，左孩子

    return mi2, Node2_list2

#递归构建Huffman树
def generateHuffman(source):
    print("generateHuffman()")

    while True:
        child, data = min2(source)

        left = child[1]  #左孩子，词频较大的
        right = child[0]   # 右孩子，词频较小的
        sum = left.value + right.value   #中间向量

        father = Nodes(sum)
        father.left = left   # 左孩子
        father.right = right   #右孩子

        child[0].father = father  # 给孩子找父亲
        child[1].father = father  # 给孩子找父亲

        father.weight = np.zeros(m_length)   # syn1初始化

        if data == []:
            return father

        data.append(father)
        source = data

#词向量初始化，使用字典将单词和词向量连接在一起
def initwords():
    print("initwords()")

    for item in words_diff:
        word_vec[item] = np.random.random((1, m_length))[0]/m_length
        #代表生成 1 行 m_length 列的浮点数，浮点数都是从0-1中随机。
    pass

#得到2*window_length 个上下文词
def get_windows(center):
    res_context = []

    for index in range(1, len(words_corpus)-1): # 前面的词
        i = center - index
        if i >= 0:
            if words_corpus[i] in words_diff:
                res_context.append(words_corpus[i])

        if len(res_context) == 2 * window_length:
            break

        j = center + index
        if j <= len(words_corpus) - 1:
            if words_corpus[j] in words_diff:
                res_context.append(words_corpus[j])

        if len(res_context) == 2 * window_length:
            break

    return res_context   # 返回的是word 列表

# 得到dw和syn1
def get_path(source, root):
    print("get_path()")

    for node in source:
        vec_path = []   #向量
        code_path = []   # 编码
        #  叶子节点有编码，但没有向量
        node_tmp = node
        while node_tmp != root: #根节点有向量，但没有编码
            code_path.append(node_tmp.HuffmanCode)
            node_tmp = node_tmp.father
            vec_path.append(node_tmp)

        word = node.word
        word_Huffman_path_vec[word] = vec_path
        word_Huffman_path_code[word] = code_path

    pass


def sigmoid(inx):
    if inx >= 0:      #对sigmoid函数的优化，避免了出现极大的数据溢出
        return 1.0 /(1 + np.exp(-inx))
    else:
        return np.exp(inx)/(1 + np.exp(inx))


def SK_HS(root, source):
    print("SK_HS")

    get_path(source, root)

    for index in range(len(words_corpus)):

        # 只有词频超过这个阀值的词才能被训练
        if words_corpus[index] in words_diff:

            context = get_windows(index)   # 上下文
            word_center = words_corpus[index]   # 中心词

            for item1 in context:

                #上下文的非叶子权重向量和Huffman编码
                vec_path = word_Huffman_path_vec[item1]
                code_path = word_Huffman_path_code[item1]

                global neule   # e
                neule = np.zeros(m_length)

                Lw_length = len(vec_path)

                #参数更新
                for i in range(Lw_length):
                    j = Lw_length - i - 1

                    x_vec = np.dot(word_vec[word_center], vec_path[j].weight)
                    q = sigmoid(x_vec)

                    g = p_learning * (1 - code_path[j] - q)

                    neule = neule + g * vec_path[j].weight

                    vec_path[j].weight = vec_path[j].weight + g * word_vec[word_center]

                word_vec[word_center] = word_vec[word_center] + neule

                # 不知道 += 像上面这样写对不对
    pass


if __name__ == '__main__':
    start = time.time()
    print("程序正在执行......")

    datanodes = GetNodes()
    root = generateHuffman(datanodes)
    initwords()  # 对词向量初始化
    SK_HS(root, datanodes)

    end = time.time()
    print("程序运行时间:" + str(end - start) + "s")
